{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "COMP 4705 AdvTpcs Final Project Ian Sinclair.ipynb",
      "provenance": [],
      "toc_visible": true,
      "mount_file_id": "1FydxBKIvob6KXaZbZ35DT3griv-awWA4",
      "authorship_tag": "ABX9TyOTj8m1XIVZk7iUfzwSA650",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ian-Sinclair/NLP-Multiclass-Topic-Modeling/blob/main/COMP_4705_AdvTpcs_Final_Project_Ian_Sinclair.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfekYFi_e0Dp"
      },
      "source": [
        "# Support Ticket Topic Modeling\n",
        "COMP 4705 Data Analysis- Industry\n",
        "\n",
        "Professor Dalton Crutchfield\n",
        "\n",
        "Ian Sinclair"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XpVHFP9LQBzk"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJLpM3njQv1I",
        "outputId": "be50334e-47ee-4e9d-d0cb-66573f1c7edc"
      },
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "import numpy as np\n",
        "import io\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from nltk.corpus import stopwords\n",
        "from scipy import sparse\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import SGDClassifier\n",
        "from sklearn.model_selection import StratifiedKFold\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.metrics import classification_report, confusion_matrix\n",
        "from sklearn.svm import SVC\n",
        "import sys\n",
        "import re\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "from nltk import word_tokenize, pos_tag\n",
        "from google.colab import drive\n",
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmqFD4DiQEO4"
      },
      "source": [
        "# Data Collection (Uploading)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kTcdPLwxQypJ"
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XpOhVE7MQ10_"
      },
      "source": [
        "dataset_link = []\n",
        "\n",
        "for file_index in range(7,99) :\n",
        "  copied_path = 'Classified File Path'\n",
        "  data = pd.read_csv(copied_path, usecols=['Summary',\n",
        "                                               #'Issue Type',\n",
        "                                               #'Status',\n",
        "                                               #'Project name', \n",
        "                                               #'Project type',\n",
        "                                               #'Labels',\n",
        "                                               #'Description',\n",
        "                                               #'Original estimate',\n",
        "                                               #'Custom field (Category)',\n",
        "                                               'Custom field (Job Failure Cause)',\n",
        "                                               'Custom field (Job Failure Error)',\n",
        "                                               #'Custom field (Job Failure Source)',\n",
        "                                               'Custom field (Job Failure Sub-Cause)',\n",
        "                                               #'Custom field (Job Name)',\n",
        "                                               #'Custom field (Job Title)'\n",
        "                                               ])\n",
        "  dataset_link.append(data)\n",
        "\n",
        "data = pd.concat(dataset_link)\n",
        "data\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "84TuMZEUQOx9"
      },
      "source": [
        "# Data Processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cPJihu1UcnnQ"
      },
      "source": [
        "Here we filtered out empty row information from the input document corpus and reduced the dataset to two columns, with a single feature, (the error messages) and a set of classes, (the sub-catagories)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "melVpdAkRG2j"
      },
      "source": [
        "\n",
        "messy_data = data.dropna(subset=['Custom field (Job Failure Sub-Cause)',\"Custom field (Job Failure Error)\"])\n",
        "\n",
        "filtered_data = messy_data[[\"Custom field (Job Failure Sub-Cause)\",\n",
        "                   \"Custom field (Job Failure Error)\"]]\n",
        "\n",
        "\n",
        "sub_causes_headers = []\n",
        "\n",
        "for index, row in data.iterrows() :\n",
        "  if row[\"Custom field (Job Failure Sub-Cause)\"] not in sub_causes_headers :\n",
        "    sub_causes_headers.append(row[\"Custom field (Job Failure Sub-Cause)\"])\n",
        "\n",
        "#print(sub_causes_headers)\n",
        "\n",
        "data = filtered_data\n",
        "data\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzSd26P-c--l"
      },
      "source": [
        "Initialized variables for the bag of words tokenizer, to be used in the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzxycTufRJNt"
      },
      "source": [
        "\n",
        "BoW = CountVectorizer()\n",
        "\n",
        "bag = BoW.fit_transform(data['Custom field (Job Failure Error)'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vUAqs5y2dOWK"
      },
      "source": [
        "Initialized tfidf vectorizer transformation and L2 norm preprocessing on document corpus."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oDvaVmbBRZvv",
        "outputId": "bdc3b7da-8920-470b-8e03-e68334cc1d35"
      },
      "source": [
        "tfidf = TfidfTransformer(use_idf=True, norm=None, smooth_idf=True)\n",
        "\n",
        "raw_tfidf = tfidf.fit_transform(BoW.fit_transform(data['Custom field (Job Failure Error)'])).toarray()\n",
        "raw_tfidf \n",
        "\n",
        "l2_tfidf = raw_tfidf / np.sqrt(np.sum(raw_tfidf**2))\n",
        "l2_tfidf"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       ...,\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.],\n",
              "       [0., 0., 0., ..., 0., 0., 0.]])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLM7KPsOdYvg"
      },
      "source": [
        "Initialized functions to stem words in each document in the corpus, reducing the complexity of the vocabulary matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mHO7wyp-VMRR"
      },
      "source": [
        "porter = PorterStemmer()\n",
        "\n",
        "def tokenizer(text):\n",
        "    return text.split()\n",
        "\n",
        "def tokenizer_porter(text):\n",
        "    return [porter.stem(word) for word in text.split()]\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "csK88_LUdkrH"
      },
      "source": [
        "Set up stop words so unnecessary words can be removed from the document corpus.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RwuFfqddRtCv",
        "outputId": "be116608-545e-4526-cb42-e4b0804ba604"
      },
      "source": [
        "nltk.download('stopwords')\n",
        "stop = stopwords.words('english')\n",
        "for i in data['Custom field (Job Failure Error)'] :  \n",
        "  [w for w in tokenizer_porter(i) if w not in stop]\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NKQrRVIYQXZ1"
      },
      "source": [
        "# Model Optimization and Tokenizing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VaVMEonWdvun"
      },
      "source": [
        "Initializes pipeline for bag of words, L2 norm vectorizer, and support vector classifier."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AYnQoCXmSlDC"
      },
      "source": [
        "param_grid = [{'vect__stop_words': [stop, None],\n",
        "               'vect__ngram_range': ((1, 1), (1, 2)),\n",
        "               'clf__C': [0.1, 1, 10, 100, 1000],\n",
        "               'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "               'clf__kernel': ['rbf']},\n",
        "              {'vect__ngram_range': ((1, 1), (1, 2)),\n",
        "               'vect__stop_words': [stop, None],\n",
        "               'tfidf__use_idf':[True, False],\n",
        "               'tfidf__norm':[None,'l1', 'l2'],\n",
        "               'clf__C': [0.1, 1, 10, 100, 1000],\n",
        "               'clf__gamma': [1, 0.1, 0.01, 0.001, 0.0001],\n",
        "               'clf__kernel': ['rbf']}]\n",
        "sgd_tfidf = Pipeline([('vect', BoW),\n",
        "                      ('tfidf', tfidf),\n",
        "                      ('clf', SVC())])\n",
        "\n",
        "gs_sgd_tfidf = GridSearchCV(sgd_tfidf, param_grid,\n",
        "                           scoring='accuracy',\n",
        "                           cv=3, #cv = 5\n",
        "                           verbose=2,\n",
        "                           n_jobs=-1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yyHsMh2wd3wV"
      },
      "source": [
        "Splits document corpus into training and testing sets, \n",
        "NOTE: because of the size limitations of the dataset and the number of\n",
        "classes, special care was taken to ensure the training set contained at  least one entry for every class."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "S0RGJTFBS7ta",
        "outputId": "1741bb57-8d9b-44e0-f893-e39d93e6ec4b"
      },
      "source": [
        "\n",
        "X_train = []\n",
        "y_train = []\n",
        "X_test = []\n",
        "y_test = []\n",
        "\n",
        "\n",
        "for index, row in data.iterrows() :\n",
        "  if row['Custom field (Job Failure Sub-Cause)'] not in y_test :\n",
        "    X_test.append(row['Custom field (Job Failure Error)'])\n",
        "    y_test.append(row['Custom field (Job Failure Sub-Cause)'])\n",
        "  else :\n",
        "    X_train.append(row['Custom field (Job Failure Error)'])\n",
        "    y_train.append(row['Custom field (Job Failure Sub-Cause)'])\n",
        "\n",
        "print(len(X_train))\n",
        "\n",
        "for index in range(0,78) :\n",
        "  X_test.append(X_train[index])\n",
        "  y_test.append(y_train[index])\n",
        "  X_train.pop(index)\n",
        "  y_train.pop(index)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "478\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BLRKiUL3eM0q"
      },
      "source": [
        "Fits the training data to the pipline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UiwyBtELTAHe",
        "outputId": "1f91db68-7356-414d-cd34-85b9d6e6e480"
      },
      "source": [
        "gs_sgd_tfidf.fit(X_train, y_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 3 folds for each of 700 candidates, totalling 2100 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/sklearn/model_selection/_split.py:667: UserWarning: The least populated class in y has only 2 members, which is less than n_splits=3.\n",
            "  % (min_groups, self.n_splits)), UserWarning)\n",
            "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=-1)]: Done  37 tasks      | elapsed:   11.5s\n",
            "[Parallel(n_jobs=-1)]: Done 158 tasks      | elapsed:   44.9s\n",
            "[Parallel(n_jobs=-1)]: Done 361 tasks      | elapsed:  1.7min\n",
            "[Parallel(n_jobs=-1)]: Done 644 tasks      | elapsed:  2.9min\n",
            "[Parallel(n_jobs=-1)]: Done 1009 tasks      | elapsed:  4.4min\n",
            "[Parallel(n_jobs=-1)]: Done 1454 tasks      | elapsed:  6.3min\n",
            "[Parallel(n_jobs=-1)]: Done 1981 tasks      | elapsed:  8.6min\n",
            "[Parallel(n_jobs=-1)]: Done 2100 out of 2100 | elapsed:  9.1min finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "GridSearchCV(cv=3, error_score=nan,\n",
              "             estimator=Pipeline(memory=None,\n",
              "                                steps=[('vect',\n",
              "                                        CountVectorizer(analyzer='word',\n",
              "                                                        binary=False,\n",
              "                                                        decode_error='strict',\n",
              "                                                        dtype=<class 'numpy.int64'>,\n",
              "                                                        encoding='utf-8',\n",
              "                                                        input='content',\n",
              "                                                        lowercase=True,\n",
              "                                                        max_df=1.0,\n",
              "                                                        max_features=None,\n",
              "                                                        min_df=1,\n",
              "                                                        ngram_range=(1, 1),\n",
              "                                                        preprocessor=None,\n",
              "                                                        stop_words=None,\n",
              "                                                        strip_accents=None,\n",
              "                                                        token_pattern='(?u)...\n",
              "                          'vect__ngram_range': ((1, 1), (1, 2)),\n",
              "                          'vect__stop_words': [['i', 'me', 'my', 'myself', 'we',\n",
              "                                                'our', 'ours', 'ourselves',\n",
              "                                                'you', \"you're\", \"you've\",\n",
              "                                                \"you'll\", \"you'd\", 'your',\n",
              "                                                'yours', 'yourself',\n",
              "                                                'yourselves', 'he', 'him',\n",
              "                                                'his', 'himself', 'she',\n",
              "                                                \"she's\", 'her', 'hers',\n",
              "                                                'herself', 'it', \"it's\", 'its',\n",
              "                                                'itself', ...],\n",
              "                                               None]}],\n",
              "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
              "             scoring='accuracy', verbose=2)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cgIndmgjeSlO"
      },
      "source": [
        "Records the CV accuracy from the training data after leaving the pipeline."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4X_Y4bs3TF3M",
        "outputId": "ae267121-da6c-428c-9662-444af76f1eeb"
      },
      "source": [
        "print('Best parameter set: %s ' % gs_sgd_tfidf.best_params_)\n",
        "print('CV Accuracy: %.3f' % gs_sgd_tfidf.best_score_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Best parameter set: {'clf__C': 100, 'clf__gamma': 0.01, 'clf__kernel': 'rbf', 'tfidf__norm': 'l2', 'tfidf__use_idf': False, 'vect__ngram_range': (1, 2), 'vect__stop_words': ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]} \n",
            "CV Accuracy: 0.428\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eSmEnsm8eYGz"
      },
      "source": [
        "Final accuracy of the testing data based on the support vector machine classifier from the training data."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J0G_dYxSTLNM",
        "outputId": "6a4fd033-9801-434b-a166-3f8fd11d05f8"
      },
      "source": [
        "clf = gs_sgd_tfidf.best_estimator_\n",
        "print('Test Accuracy: %.3f' % clf.score(X_test, y_test))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Test Accuracy: 0.348\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}